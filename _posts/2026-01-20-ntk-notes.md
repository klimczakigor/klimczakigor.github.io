---
layout: post
title: "[notes] NTK analysis for an MLP with one hidden layer"
---

These notes are a lightweight sketch of the neural tangent kernel for a shallow network.
We use them to show math rendering and the shorter excerpt style on the homepage.

For a network $f_\theta(x)$, the NTK is

$$
K(x, x') = \nabla_\theta f_\theta(x)^\top \nabla_\theta f_\theta(x').
$$
